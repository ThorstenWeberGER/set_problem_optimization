{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c182f882",
   "metadata": {},
   "source": [
    "## Das Optimierungsproblem von Standorten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb39546",
   "metadata": {},
   "source": [
    "Es geht um die Auswahl von Standorten, um Kunden gut zu betreuen. Für Logistikzentren, für Nahversorgung, Planung von Ärztezentren etc.\n",
    "\n",
    "Konzeptionell:\n",
    "- Kundendistanz max. 50km zu Standort\n",
    "- mind. 90% Kunden abdecken\n",
    "- Bevorzugung der Top200 Städte in Deutschland (Potenzielle Kundschaft)\n",
    "- Bevorzugung aktuelle Kundschaft der Stadt (Aktuelle Kundschaft)\n",
    "- Minimieren der Standorte als Ziel\n",
    "\n",
    "Im folgenden werden genutzt:\n",
    "- der Optimierer PulP. Er ist Industrie Standard und Open Source. \n",
    "- PGeoCode zur Identifizierung von Geo-Coordinaten auf Grundlage von PLZ-Codes\n",
    "\n",
    "Optimierung funktioniert über Lineare Programmierung\n",
    "- Zielfunktion ist Kostenminimierung\n",
    "- Kosten entstehen durch Standorte\n",
    "- Nebenbedingung beschreibt, wie die Kunden abgedeckt sein müssen (Beispiel 1: jeder Kunde mind. 1 Standort. Beispiel 2: mind. 90% der Kunden mit mind. 1 Standort)\n",
    "- Standortdistanz max. 50km zu Kundenort\n",
    "- Kosten-Malus für Nebenstädte, Bevorzugung der Top 200 Städte Deutschlands\n",
    "\n",
    "Note:\n",
    "- die PLZ der candidates (potenzielle Standorte) sollten die zentralen PLZ der Stadt sein, damit die Abstände zu Kunden und anderen Städten korrekt berechnet werden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b02b4e",
   "metadata": {},
   "source": [
    "### Einfache Einsteigerfreundliche Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f695b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Erstelle Mock-Daten...\n",
      "2. Geocoding der Standorte...\n",
      "3. Berechne Distanzen und Coverage-Logik...\n",
      "-> 0 PLZ-Gebiete können nicht abgedeckt werden (kein Standort in 50.0km).\n",
      "4. Starte Solver...\n",
      "------------------------------\n",
      "Status: Optimal\n",
      "------------------------------\n",
      "Ausgewählte optimale Standorte (Anzahl: 2):\n",
      "[x] Frankfurt am Main (60311) (Top-City)\n",
      "[x] Fulda (36037) (Local)\n",
      "------------------------------\n",
      "Interpretation:\n",
      "Offenbach und Rüsselsheim sollten NICHT gewählt sein, wenn Frankfurt sie abdeckt.\n",
      "Fulda sollte gewählt sein, da es zu weit weg von Frankfurt ist.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pulp\n",
    "import pgeocode\n",
    "\n",
    "# --- KONFIGURATION ---\n",
    "MAX_DISTANCE_KM = 50.0  # Harte Grenze (ggf. auf 38 reduzieren für \"echte Fahrstrecke\")\n",
    "METROPOLIS_BONUS = 0.8  # Kostenfaktor für Top-Städte (niedriger = bevorzugt)\n",
    "STANDARD_COST = 1.0     # Kostenfaktor für normale Standorte\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. MOCK DATEN ERSTELLEN (Simulation deiner Ausgangslage)\n",
    "# ---------------------------------------------------------\n",
    "print(\"1. Erstelle Mock-Daten...\")\n",
    "\n",
    "# Simulierte Top Städte (Beispiel Rhein-Main)\n",
    "top_cities_data = {\n",
    "    'city': ['Frankfurt am Main', 'München', 'Hamburg', 'Köln', 'Mainz', 'Wiesbaden'],\n",
    "    'plz': ['60311', '80331', '20095', '50667', '55116', '65185'],\n",
    "    'is_top_200': [True, True, True, True, True, True]\n",
    "}\n",
    "df_candidates = pd.DataFrame(top_cities_data)\n",
    "\n",
    "# Simulierte Kunden (Aggregiert auf PLZ5)\n",
    "# Hanau (nahe FFM), Offenbach (nahe FFM), Rüsselsheim (Mitte), Fulda (weit weg)\n",
    "customer_data = {\n",
    "    'city': ['Hanau', 'Offenbach', 'Rüsselsheim', 'Fulda', 'Frankfurt am Mainz'],\n",
    "    'plz5': ['63450', '63065', '65428', '36037', '60311'], # Hanau, Offenbach, Rüsselsheim, Fulda, FFM\n",
    "    'customer_count': [500, 800, 400, 200, 1000]\n",
    "}\n",
    "df_demand = pd.DataFrame(customer_data)\n",
    "\n",
    "# Hinzufügen Kunden-Standorte als potenzielle Kandidaten und De-Duplizieren (falls schon in top200 liste)\n",
    "additional_candidates = pd.DataFrame({\n",
    "    'city': customer_data['city'],\n",
    "    'plz': customer_data['plz5'],\n",
    "    'is_top_200': [False, False, False, False, True]\n",
    "})\n",
    "df_candidates = pd.concat([df_candidates, additional_candidates], ignore_index=True).drop_duplicates(subset=['plz'])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. GEOCODING (PLZ -> Lat/Lon)\n",
    "# ---------------------------------------------------------\n",
    "print(\"2. Geocoding der Standorte...\")\n",
    "nomi = pgeocode.Nominatim('de')\n",
    "\n",
    "def get_coords(plz_series):\n",
    "    # pgeocode query_postal_code erwartet Strings\n",
    "    geo = nomi.query_postal_code(plz_series.astype(str).tolist())\n",
    "    return geo[['latitude', 'longitude']].values\n",
    "\n",
    "# Koordinaten für Kandidaten und Demand Points holen\n",
    "df_candidates[['lat', 'lon']] = get_coords(df_candidates['plz'])\n",
    "df_demand[['lat', 'lon']] = get_coords(df_demand['plz5'])\n",
    "\n",
    "# Bereinigen von ungültigen PLZs (falls pgeocode nichts findet)\n",
    "df_candidates.dropna(subset=['lat'], inplace=True)\n",
    "df_demand.dropna(subset=['lat'], inplace=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. DISTANZ-MATRIX & PRE-FILTERING\n",
    "# ---------------------------------------------------------\n",
    "print(\"3. Berechne Distanzen und Coverage-Logik...\")\n",
    "\n",
    "def haversine_vectorized(lat1, lon1, lat2, lon2):\n",
    "    # Vektorisierte Haversine Formel für Performance\n",
    "    R = 6371  # Erdradius in km\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1)*np.cos(phi2) * np.sin(dlambda/2)**2\n",
    "    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "# Dictionary: Demand_PLZ -> Liste [Candidate_Index, Candidate_Index, ...]\n",
    "# Nur Kandidaten < 50km werden hier eingetragen!\n",
    "coverage_map = {} \n",
    "uncoverable_demand = []\n",
    "\n",
    "# koordinaten in numpy arrays für vektorisierte Berechnung überführen, weil schneller\n",
    "candidates_lat = df_candidates['lat'].values\n",
    "candidates_lon = df_candidates['lon'].values\n",
    "\n",
    "# Für jeden Demand Point die Distanzen zu allen Kandidaten berechnen und standorte <= 50km speichern\n",
    "for idx, row in df_demand.iterrows():\n",
    "    # Distanz von DIESEM Kunden zu ALLEN Kandidaten\n",
    "    dists = haversine_vectorized(row['lat'], row['lon'], candidates_lat, candidates_lon)\n",
    "    \n",
    "    # Filter: Wer ist nah genug?\n",
    "    # Note: np.where gibt np array zurück, wir holen uns nur die Indices die liste\n",
    "    valid_indices = np.where(dists <= MAX_DISTANCE_KM)[0]\n",
    "\n",
    "    # Note: Falls alle Kundenstandorte auch den Candidate_Standorten hinzugefügt wurden,\n",
    "    # macht diese Überprüfung keinen Sinn mehr, da jeder Standort sich selbst abdeckt.\n",
    "    # Evtl. wollen wir später Kundenstandorte ausschließen. Also lassen ich die Überprüfung drin.\n",
    "    if len(valid_indices) > 0:\n",
    "        coverage_map[idx] = valid_indices\n",
    "    else:\n",
    "        uncoverable_demand.append(row['plz5'])\n",
    "\n",
    "print(f\"-> {len(uncoverable_demand)} PLZ-Gebiete können nicht abgedeckt werden (kein Standort in {MAX_DISTANCE_KM}km).\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. OPTIMIERUNG (PuLP Solver)\n",
    "# ---------------------------------------------------------\n",
    "print(\"4. Starte Solver...\")\n",
    "\n",
    "# LP Problem initialisieren. Ziel: Minimiere Kosten\n",
    "prob = pulp.LpProblem(\"Facility_Location_Optimization\", pulp.LpMinimize)\n",
    "\n",
    "# Entscheidungsvariablen: Welche Candidate Site machen wir auf? (kann für jeden stadt_index 0 oder 1 annehmen für nein, ja)\n",
    "# der solver wird diese variablen so setzen, dass die Zielfunktion minimiert wird unter einhaltung der constraints\n",
    "candidate_indices = df_candidates.index.tolist()\n",
    "location_vars = pulp.LpVariable.dicts(\"Open_Site\", candidate_indices, 0, 1, pulp.LpBinary)\n",
    "\n",
    "# --- ZIELFUNKTION ---\n",
    "# Wir minimieren die Kosten, dabei...\n",
    "# ...bevorzugen wir TOP200 Städten (wegen Kundenpotenzial)\n",
    "# ...bevorzugen wir Städte mit vielen Kunden (aktuelle Kundschaft) \n",
    "\n",
    "# Die Gesamtkosten sind die Summe aus den Kosten die für jede Stadt entstehen\n",
    "# Also erstellen wir hier eine Liste mit den Kosten pro Stadt \n",
    "# und summieren diese dann in der Zielfunktion\n",
    "costs = []\n",
    "for idx in candidate_indices: \n",
    "    cost_factor = (\n",
    "        METROPOLIS_BONUS \n",
    "        if df_candidates.at[idx, 'is_top_200'] # is top_city\n",
    "        else STANDARD_COST\n",
    "    )\n",
    "    # Nachfrage-Bonus: Wenn am Standort Nachfrage ist, mache die Kosten niedriger \n",
    "    demand_here = (\n",
    "        df_demand[df_demand['plz5'] == df_candidates.at[idx, 'plz']]\n",
    "        ['customer_count'].sum()\n",
    "    )\n",
    "    if demand_here > 0:\n",
    "        cost_factor = cost_factor / np.sqrt(demand_here)  \n",
    "        # Niedrigere Kosten für hohe Nachfrage \n",
    "        # skaliere demand_here um den Effekt anzupassen (e.g. sqrt(demand))\n",
    "    \n",
    "    costs.append(location_vars[idx] * cost_factor) # locations_vars[idx] ist 0 oder 1 und eine Solver-Variable\n",
    "\n",
    "# Zielfunktion setzen. Merke: pulp.lpSum fügt alle Kostenfunktionen pro Stadt zusammen. \n",
    "prob += pulp.lpSum(costs)\n",
    "\n",
    "# --- CONSTRAINTS ---\n",
    "# Jeder abdeckbare Kunde MUSS von mind. 1 Standort abgedeckt sein\n",
    "# Merke: coverage_map enthält nur abdeckbare Kunden und potenzielle Standorte\n",
    "# Merke: location_vars ist die Solver-Variable, die angibt, ob ein Standort geöffnet wird, kann 0 oder 1 annehmen\n",
    "for demand_idx, site_idxs in coverage_map.items():\n",
    "    prob += pulp.lpSum([location_vars[site_idx] for site_idx in site_idxs]) >= 1\n",
    "\n",
    "# Lösen ohne Ausgabe des Verlaufs\n",
    "prob.solve(pulp.PULP_CBC_CMD(msg=False))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. ERGEBNIS AUSWERTUNG\n",
    "# ---------------------------------------------------------\n",
    "print(\"-\" * 30)\n",
    "print(f\"Status: {pulp.LpStatus[prob.status]}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "selected_sites = []\n",
    "# add open locations (value=1) to selected_sites\n",
    "for site_idx in candidate_indices:\n",
    "    if location_vars[site_idx].value() == 1.0:\n",
    "        city_name = df_candidates.at[site_idx, 'city']\n",
    "        city_plz = df_candidates.at[site_idx, 'plz']\n",
    "        is_top = df_candidates.at[site_idx, 'is_top_200']\n",
    "        selected_sites.append(f\"{city_name} ({city_plz}) ({'Top-City' if is_top else 'Local'})\")\n",
    "\n",
    "selected_sites.sort()\n",
    "\n",
    "print(f\"Ausgewählte optimale Standorte (Anzahl: {len(selected_sites)}):\")\n",
    "for site in selected_sites:\n",
    "    print(f\"[x] {site}\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"Interpretation:\")\n",
    "print(\"Offenbach und Rüsselsheim sollten NICHT gewählt sein, wenn Frankfurt sie abdeckt.\")\n",
    "print(\"Fulda sollte gewählt sein, da es zu weit weg von Frankfurt ist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc5c38",
   "metadata": {},
   "source": [
    "### Saubere schnellere konfigurierbarere Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979ecf5",
   "metadata": {},
   "source": [
    "Den obigen Code habe ich in Claude gesendet mit der bitte ihn zu optimieren (bessere struktur, einfacher verständlich, gut commentiert und production ready). Nach einigen Iterationen kam das hier raus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11cf6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Status: Optimal\n",
      "\n",
      "Gewählte Standorte:\n",
      " - Frankfurt am Main \t\t(Pop: 764,000, Kunden (gewichtet): 2300.0 (2218.7))\n",
      " - München \t\t(Pop: 1,488,000, Kunden (gewichtet): 1200.0 (1200.0))\n",
      " - Hamburg \t\t(Pop: 1,853,000, Kunden (gewichtet): 1100.0 (1100.0))\n",
      " - Berlin \t\t(Pop: 3,664,000, Kunden (gewichtet): 1500.0 (1500.0))\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# PURPOSE: Minimize locations to cover a specified amount of customers \n",
    "# =========================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pulp\n",
    "import pgeocode\n",
    "import folium\n",
    "import webbrowser\n",
    "import logging\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "\n",
    "# == CONFIGURATION ========================================================\n",
    "CONFIG = {\n",
    "    'max_distance_km': 35.0, # reduced cause real driving time is relevant to customer\n",
    "    'service_level': 0.90,\n",
    "    'cost_top_city': 0.8,\n",
    "    'cost_standard': 1.0,\n",
    "    'customer_bonus': 0.2,\n",
    "    'prestige_bonus': 0.1,\n",
    "    'earth_radius_km': 6371.0,\n",
    "    'decay_start_km': 10.0,\n",
    "    'min_weight_at_max': 0.5,\n",
    "    'candidates_path': 'c:/Users/Thorsten/german_cities_clean_utf8.csv',\n",
    "    'demand_path': 'c:/Users/Thorsten/90000_customers.csv',\n",
    "    'results_path': 'c:/Users/Thorsten/optimized_locations_list.csv',\n",
    "    'log_file': 'c:/Users/Thorsten/optimization_process.log'\n",
    "}\n",
    "\n",
    "# == LOGGING ================================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - [%(filename)s] - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(CONFIG['log_file'], mode='a', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def log_separator():\n",
    "    try:\n",
    "        filename = os.path.basename(__file__)\n",
    "    except NameError:\n",
    "        filename = \"Interactive_Session\"\n",
    "    logging.info(\"=\"*60)\n",
    "    logging.info(f\"START: Execution of {filename}\")\n",
    "    logging.info(\"=\"*60)\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING & PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "def read_data():\n",
    "    logging.info(\"Step 1: Reading input CSV files...\")\n",
    "    try:\n",
    "        df_candidates = pd.read_csv(CONFIG['candidates_path'], dtype={'plz': str})\n",
    "        threshold = df_candidates['population_total'].nlargest(200).min()\n",
    "        df_candidates['is_top_200'] = df_candidates['population_total'] >= threshold\n",
    "        \n",
    "        df_demand = pd.read_csv(CONFIG['demand_path'])\n",
    "        if 'plz5' not in df_demand.columns and 'plz-nummer' in df_demand.columns:\n",
    "            df_demand = df_demand.rename(columns={'plz-nummer': 'plz5'})\n",
    "        \n",
    "        df_demand['plz5'] = df_demand['plz5'].astype(str)\n",
    "        logging.info(f\"Loaded {len(df_candidates)} candidates and {len(df_demand)} demand points.\")\n",
    "        return df_candidates, df_demand\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to read data: {e}\")\n",
    "        raise\n",
    "\n",
    "def add_coordinates(df, plz_column):\n",
    "    logging.info(f\"Enriching coordinates for {plz_column}...\")\n",
    "    geo = pgeocode.Nominatim('de')\n",
    "    valid_geo_data = geo._data.dropna(subset=['latitude', 'longitude'])\n",
    "    valid_zip_set = set(valid_geo_data['postal_code'].unique())\n",
    "\n",
    "    df[plz_column] = df[plz_column].str.replace('.0', '', regex=False).str.zfill(5)\n",
    "    initial_count = len(df)\n",
    "    df = df[df[plz_column].isin(valid_zip_set)].copy()\n",
    "    \n",
    "    geo_info = geo.query_postal_code(df[plz_column].tolist())\n",
    "    df['lat'] = geo_info['latitude'].values\n",
    "    df['lon'] = geo_info['longitude'].values\n",
    "    df[['lat_rad', 'lon_rad']] = np.radians(df[['lat', 'lon']])\n",
    "    \n",
    "    logging.info(f\"Geocoding finished. {len(df)}/{initial_count} locations valid.\")\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# =============================================================================\n",
    "# LOGIC & OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_coverage(df_demand, df_candidates, max_distance):\n",
    "    logging.info(\"Calculating catchment areas and weights...\")\n",
    "    coords_demand = df_demand[['lat_rad', 'lon_rad']].to_numpy()\n",
    "    coords_candidates = df_candidates[['lat_rad', 'lon_rad']].to_numpy()\n",
    "    dist_matrix = haversine_distances(coords_demand, coords_candidates) * CONFIG['earth_radius_km']\n",
    "    \n",
    "    location_stats = {}\n",
    "    coverage = {}\n",
    "    demand_col = 'customer_count'\n",
    "    max_pop = df_candidates['population_total'].max()\n",
    "\n",
    "    for s_idx in range(len(df_candidates)):\n",
    "        c_sum_total = 0\n",
    "        w_sum_weighted = 0\n",
    "        reachable_indices = []\n",
    "        for k_idx in range(len(df_demand)):\n",
    "            d = dist_matrix[k_idx, s_idx]\n",
    "            if d <= max_distance:\n",
    "                reachable_indices.append(k_idx)\n",
    "                count = df_demand.iloc[k_idx][demand_col]\n",
    "                if d <= CONFIG['decay_start_km']:\n",
    "                    weight = 1.0\n",
    "                else:\n",
    "                    dist_ratio = (d - CONFIG['decay_start_km']) / (max_distance - CONFIG['decay_start_km'])\n",
    "                    weight = 1.0 - dist_ratio * (1.0 - CONFIG['min_weight_at_max'])\n",
    "                c_sum_total += count\n",
    "                w_sum_weighted += count * weight\n",
    "        \n",
    "        loc_id = df_candidates.index[s_idx]\n",
    "        coverage[loc_id] = reachable_indices\n",
    "        location_stats[loc_id] = {\n",
    "            'customers_total': float(c_sum_total),\n",
    "            'customers_weighted': float(w_sum_weighted),\n",
    "            'pop_factor': df_candidates.iloc[s_idx]['population_total'] / max_pop\n",
    "        }\n",
    "\n",
    "    all_weighted = [s['customers_weighted'] for s in location_stats.values()]\n",
    "    mx, mn = (max(all_weighted), min(all_weighted)) if all_weighted else (1, 0)\n",
    "    for loc_id in location_stats:\n",
    "        location_stats[loc_id]['customer_factor'] = (location_stats[loc_id]['customers_weighted'] - mn) / (mx - mn) if mx > mn else 1.0\n",
    "            \n",
    "    cust_to_loc = {k_idx: [loc_id for loc_id, ids in coverage.items() if k_idx in ids] for k_idx in range(len(df_demand))}\n",
    "    return cust_to_loc, location_stats\n",
    "\n",
    "def optimize_locations(df_demand, df_candidates, coverage, location_stats):\n",
    "    logging.info(\"Starting PuLP optimization...\")\n",
    "    problem = pulp.LpProblem(\"Location_Optimization\", pulp.LpMinimize)\n",
    "    is_opened = pulp.LpVariable.dicts(\"loc\", df_candidates.index, cat=pulp.LpBinary)\n",
    "    is_served = pulp.LpVariable.dicts(\"cust\", df_demand.index, cat=pulp.LpBinary)\n",
    "    \n",
    "    costs = []\n",
    "    for i in df_candidates.index:\n",
    "        base_cost = CONFIG['cost_top_city'] if df_candidates.at[i, 'is_top_200'] else CONFIG['cost_standard']\n",
    "        bonus = (location_stats[i]['customer_factor'] * CONFIG['customer_bonus']) + \\\n",
    "                (location_stats[i]['pop_factor'] * CONFIG['prestige_bonus'])\n",
    "        costs.append(is_opened[i] * (base_cost - bonus))\n",
    "    \n",
    "    problem += pulp.lpSum(costs)\n",
    "    for k in df_demand.index:\n",
    "        problem += pulp.lpSum(is_opened[s] for s in coverage.get(k, [])) >= is_served[k]\n",
    "    \n",
    "    demand_col = 'customer_count'\n",
    "    min_required = df_demand[demand_col].sum() * CONFIG['service_level']\n",
    "    problem += pulp.lpSum(is_served[i] * df_demand.at[i, demand_col] for i in df_demand.index) >= min_required\n",
    "    \n",
    "    problem.solve(pulp.PULP_CBC_CMD(msg=False))\n",
    "    logging.info(f\"Optimization Status: {pulp.LpStatus[problem.status]}\")\n",
    "    return problem, is_opened, is_served # Return problem to check status in main\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT & VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def export_results_to_csv(df_candidates, is_opened, stats, output_path):\n",
    "    logging.info(f\"Generating results CSV: {output_path}\")\n",
    "    opened_indices = [idx for idx in df_candidates.index if is_opened[idx].value() > 0.5]\n",
    "    \n",
    "    export_data = []\n",
    "    for idx in opened_indices:\n",
    "        row = df_candidates.loc[idx]\n",
    "        export_data.append({\n",
    "            'city_name': row['city_name'],\n",
    "            'plz': row['plz'],\n",
    "            'city_type': 'Top 200' if row['is_top_200'] else 'Standard',\n",
    "            'patients_covered_weighted': round(stats[idx]['customers_weighted'], 2),\n",
    "            'patients_covered_total': int(stats[idx]['customers_total'])\n",
    "        })\n",
    "    \n",
    "    df_results = pd.DataFrame(export_data)\n",
    "    df_results = df_results.sort_values(by='patients_covered_total', ascending=False)\n",
    "    df_results.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    logging.info(f\"Export successful. {len(df_results)} locations exported.\")\n",
    "\n",
    "def visualize_and_open(df_candidates, df_demand, is_opened, is_served, stats):\n",
    "    \"\"\"\n",
    "    Creates a focused interactive dashboard:\n",
    "    - Only opened locations visible.\n",
    "    - Detailed Pop-ups.\n",
    "    - Permanent legend with KPIs.\n",
    "    \"\"\"\n",
    "    logging.info(\"Creating final dashboard map...\")\n",
    "    \n",
    "    # Initialize map (Centered on Germany)\n",
    "    m = folium.Map(location=[51.1657, 10.4515], zoom_start=6, control_scale=True)\n",
    "    \n",
    "    demand_col = 'customer_count'\n",
    "    total_customers_data = int(df_demand[demand_col].sum())\n",
    "    opened_indices = [idx for idx in df_candidates.index if is_opened[idx].value() > 0.5]\n",
    "    num_opened = len(opened_indices)\n",
    "    \n",
    "    covered_customers = sum(\n",
    "        df_demand.at[idx, demand_col] \n",
    "        for idx in df_demand.index \n",
    "        if is_served[idx].value() > 0.5\n",
    "    )\n",
    "\n",
    "    # 1. Plot opened locations\n",
    "    for idx in opened_indices:\n",
    "        row = df_candidates.loc[idx]\n",
    "        \n",
    "        popup_html = f\"\"\"\n",
    "        <div style=\"font-family: Arial; width: 220px;\">\n",
    "            <h4 style=\"margin-bottom:5px;\">{row['city_name']}</h4>\n",
    "            <hr>\n",
    "            <b>Status:</b> Opened<br>\n",
    "            <b>Total Customers in Reach:</b> {stats[idx]['customers_total']:.0f}<br>\n",
    "            <b>Weighted Potential:</b> {stats[idx]['customers_weighted']:.1f}<br>\n",
    "            <b>City Type:</b> {'Top 200' if row['is_top_200'] else 'Standard'}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        folium.Marker(\n",
    "            [row['lat'], row['lon']], \n",
    "            icon=folium.Icon(color='blue', icon='shopping-cart', prefix='fa'), \n",
    "            popup=folium.Popup(popup_html, max_width=250)\n",
    "        ).add_to(m)\n",
    "        \n",
    "        # Visualize catchment radius\n",
    "        folium.Circle(\n",
    "            [row['lat'], row['lon']], \n",
    "            radius=CONFIG['max_distance_km'] * 1000, \n",
    "            color='blue', \n",
    "            fill=True, \n",
    "            fill_opacity=0.1,\n",
    "            weight=1\n",
    "        ).add_to(m)\n",
    "\n",
    "    # 2. Add Permanent Legend (HTML/CSS)\n",
    "    legend_html = f'''\n",
    "    <div style=\"\n",
    "        position: fixed; \n",
    "        bottom: 50px; right: 50px; width: 300px; height: 160px; \n",
    "        background-color: white; border:2px solid grey; z-index:9999; font-size:14px;\n",
    "        padding: 15px; border-radius: 10px; box-shadow: 2px 2px 5px rgba(0,0,0,0.3);\n",
    "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "        \">\n",
    "        <b style=\"font-size: 16px;\">Location Optimization Dashboard</b><br>\n",
    "        <hr style=\"margin: 10px 0;\">\n",
    "        <i class=\"fa fa-users\" style=\"color:navy\"></i> Total Customers: <b>{total_customers_data:,}</b><br>\n",
    "        <i class=\"fa fa-map-marker\" style=\"color:blue\"></i> Opened Locations: <b>{num_opened}</b><br>\n",
    "        <i class=\"fa fa-check-circle\" style=\"color:green\"></i> Covered Customers: <b>{int(covered_customers):,}</b><br>\n",
    "        <i class=\"fa fa-pie-chart\" style=\"color:orange\"></i> Actual Service Level: <b>{(covered_customers/total_customers_data)*100:.1f}%</b>\n",
    "    </div>\n",
    "    '''\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "    # 3. Save and Open\n",
    "    user_home = os.path.expanduser(\"~\")\n",
    "    map_path = os.path.join(user_home, \"location_optimization_dashboard.html\")\n",
    "    m.save(map_path)\n",
    "    \n",
    "    logging.info(f\"Dashboard saved: {map_path}\")\n",
    "    logging.info(f\"Result: {num_opened} locations cover {int(covered_customers)} customers.\")\n",
    "    webbrowser.open('file://' + os.path.realpath(map_path))\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    # 1. Logging start\n",
    "    log_separator()\n",
    "    logging.info(\"--- Optimization Process Started ---\")\n",
    "    \n",
    "    # 2. Data loading an enrichment\n",
    "    df_candidates, df_demand = read_data()\n",
    "    df_candidates = add_coordinates(df_candidates, 'plz')\n",
    "    df_demand = add_coordinates(df_demand, 'plz5')\n",
    "    \n",
    "    # 3. Coverage calculation and optimization\n",
    "    coverage, stats = calculate_coverage(df_demand, df_candidates, CONFIG['max_distance_km'])\n",
    "    # ERROR FIX: We now receive the problem object to check its status properly\n",
    "    problem, is_opened, is_served = optimize_locations(df_demand, df_candidates, coverage, stats)\n",
    "\n",
    "    # 4. Export and Visualization only if optimal solution found\n",
    "    if pulp.LpStatus[problem.status] == 'Optimal':\n",
    "        visualize_and_open(df_candidates, df_demand, is_opened, is_served, stats)\n",
    "        export_results_to_csv(df_candidates, is_opened, stats, CONFIG['results_path'])\n",
    "    else:\n",
    "        logging.error(f\"Solution status: {pulp.LpStatus[problem.status]}. Export and Visualization skipped.\")\n",
    "    \n",
    "    # 5. Logging completion\n",
    "    logging.info(\"--- Optimization Process Finished Successfully ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b76e5a",
   "metadata": {},
   "source": [
    "## Distanzberechnungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b26241",
   "metadata": {},
   "source": [
    "Wie funktionieren sie, welche Unterschiede gibt es."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624550b",
   "metadata": {},
   "source": [
    "### Standard Haversine Berechnung: 1-1 Distanz\n",
    "\n",
    "Die Bibliothek erwartet Koordinaten als Tupel im Format (Breitengrad, Längengrad). Mit der Standard Haversine Funktion kann man stets nur Distanz von zwei Punkten berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e4e318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Distanz beträgt 504.42 km\n"
     ]
    }
   ],
   "source": [
    "from haversine import haversine, Unit\n",
    "\n",
    "# Beispiel-Koordinaten (Latitude, Longitude)\n",
    "# Ersetze diese Variablen mit deinen Eingabedaten\n",
    "start_punkt = (52.5200, 13.4050) # Berlin\n",
    "end_punkt = (48.1351, 11.5820)   # München\n",
    "\n",
    "# Berechnung der Distanz\n",
    "# unit=Unit.KILOMETERS ist Standard, kann aber auch auf MILES, METERS etc. geändert werden\n",
    "distanz = haversine(start_punkt, end_punkt, unit=Unit.KILOMETERS)\n",
    "\n",
    "print(f\"Die Distanz beträgt {distanz:.2f} km\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a551e9",
   "metadata": {},
   "source": [
    "### Paarweise Berechnung von Listen\n",
    "\n",
    "Für zwei Listen mit Start und Zielpunkten eignet sich die vectorisierte Haversine Funktion. Diese ist ein vielfaches Schneller und erlaubt es Listen als Argumente zu übergeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02186e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[504.41602813 343.55653488]\n"
     ]
    }
   ],
   "source": [
    "from haversine import haversine_vector, Unit\n",
    "\n",
    "# Liste von Start-Koordinaten (Lat, Lon)\n",
    "start_punkte = [\n",
    "    (52.5200, 13.4050), # Berlin\n",
    "    (48.8566, 2.3522)   # Paris\n",
    "]\n",
    "\n",
    "# Liste von Ziel-Koordinaten (Lat, Lon)\n",
    "end_punkte = [\n",
    "    (48.1351, 11.5820), # München\n",
    "    (51.5074, -0.1278)  # London\n",
    "]\n",
    "\n",
    "# Berechnet Distanz: Berlin->München UND Paris->London\n",
    "distanzen = haversine_vector(start_punkte, end_punkte, unit=Unit.KILOMETERS)\n",
    "\n",
    "print(distanzen)\n",
    "# Ausgabe (Beispiel): [504.23, 343.56]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c395d39e",
   "metadata": {},
   "source": [
    "### Ein Startpunkt mit vielen Zielen\n",
    "\n",
    "Wenn du wissen willst, wie weit ein Standort von einer Liste anderer Standorte entfernt ist, kannst du den comb (Combination) Parameter nutzen oder die Listen entsprechend vorbereiten. Die haversine_vector Funktion ist hier sehr flexibel.\n",
    "\n",
    "Am einfachsten ist es oft, den Startpunkt in eine Liste zu packen, die so lang ist wie die Zielliste (Broadcasting), oder direkt Pandas zu nutzen, falls du DataFrames verwendest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10739209",
   "metadata": {},
   "source": [
    "### Schnelle Alternative für viele Datenpunkte\n",
    "\n",
    "Falls du mit vielen Datenpunkten oder Arrays arbeitest (z. B. in Pandas oder NumPy), ist scikit-learn oft effizienter, da es vektorisierte Operationen unterstützt. Beachte, dass scikit-learn Bogenmaß (Radians) erwartet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a38267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Distanz beträgt 504.42 km\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from math import radians\n",
    "\n",
    "# Koordinaten in Radians umwandeln\n",
    "start_in_radians = [radians(52.5200), radians(13.4050)]\n",
    "end_in_radians = [radians(48.1351), radians(11.5820)]\n",
    "\n",
    "# Ergebnis ist in Radians, muss mit Erdradius multipliziert werden (ca. 6371 km)\n",
    "result = haversine_distances([start_in_radians], [end_in_radians])\n",
    "distanz_km = result[0][0] * 6371  # Radius in km\n",
    "\n",
    "print(f\"Die Distanz beträgt {distanz_km:.2f} km\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1d8eb",
   "metadata": {},
   "source": [
    "### Erstellen einer Distanzmatrix\n",
    "\n",
    "Zwei Methoden:\n",
    "1. haversine_vector: das Argument comb=True und Übergabe einer Standorteliste als Start- und als Zielpunktliste. \n",
    "2. skikit-learn: Für viele Standorte ist das einfacher schenller\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb1820bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distanzmatrix (in km):\n",
      "               Berlin     München     Hamburg   Frankfurt\n",
      "Berlin       0.000000  504.416028  255.250519  423.528610\n",
      "München    504.416028    0.000000  612.428927  304.585539\n",
      "Hamburg    255.250519  612.428927    0.000000  392.989090\n",
      "Frankfurt  423.528610  304.585539  392.989090    0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from haversine import haversine_vector, Unit\n",
    "\n",
    "# 1. Liste der Standorte (Lat, Lon)\n",
    "standorte = [\n",
    "    (52.5200, 13.4050), # Berlin\n",
    "    (48.1351, 11.5820), # München\n",
    "    (53.5511, 9.9937),  # Hamburg\n",
    "    (50.1109, 8.6821)   # Frankfurt\n",
    "]\n",
    "\n",
    "# Namen für die bessere Lesbarkeit (optional)\n",
    "namen = [\"Berlin\", \"München\", \"Hamburg\", \"Frankfurt\"]\n",
    "\n",
    "# 2. Berechnung der Matrix\n",
    "# comb=True sorgt dafür, dass jeder Punkt aus Liste 1 mit jedem aus Liste 2 verglichen wird.\n",
    "distanz_matrix = haversine_vector(standorte, standorte, unit=Unit.KILOMETERS, comb=True)\n",
    "\n",
    "# 3. Darstellung als DataFrame\n",
    "df_matrix = pd.DataFrame(distanz_matrix, columns=namen, index=namen)\n",
    "\n",
    "print(\"Distanzmatrix (in km):\")\n",
    "print(df_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815f4928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Berlin  München  Hamburg  Frankfurt\n",
      "Berlin       0.00   504.42   255.25     423.53\n",
      "München    504.42     0.00   612.43     304.59\n",
      "Hamburg    255.25   612.43     0.00     392.99\n",
      "Frankfurt  423.53   304.59   392.99       0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from math import radians\n",
    "\n",
    "# Standorte (Lat, Lon)\n",
    "standorte = [\n",
    "    (52.5200, 13.4050), # Berlin\n",
    "    (48.1351, 11.5820), # München\n",
    "    (53.5511, 9.9937),  # Hamburg\n",
    "    (50.1109, 8.6821)   # Frankfurt\n",
    "]\n",
    "namen = [\"Berlin\", \"München\", \"Hamburg\", \"Frankfurt\"]\n",
    "\n",
    "# 1. Umwandlung in Radians (Scikit-Learn erwartet Bogenmaß!)\n",
    "# Wir nutzen np.radians für die Vektorisierung, falls standorte schon ein Array wäre,\n",
    "# hier nutzen wir eine List Comprehension für reine Python Listen:\n",
    "standorte_rad = [[radians(lat), radians(lon)] for lat, lon in standorte]\n",
    "\n",
    "# 2. Berechnung der Distanzen (Ergebnis ist in Radians)\n",
    "matrix_rad = haversine_distances(standorte_rad, standorte_rad)\n",
    "\n",
    "# 3. Umrechnung zurück in Kilometer (Erdradius ca. 6371 km)\n",
    "matrix_km = matrix_rad * 6371\n",
    "\n",
    "# 4. Schöner DataFrame\n",
    "df_matrix = pd.DataFrame(matrix_km, columns=namen, index=namen)\n",
    "\n",
    "# Runden für schönere Ausgabe\n",
    "print(df_matrix.round(2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
